{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Sequence\n",
    "\n",
    "from llama_index.core.llms.types import ChatMessage, MessageRole\n",
    "\n",
    "BOS, EOS = \"<s>\", \"</s>\"\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "IM_START, IM_END = \"<|im_start|>\", \"<|im_end|>\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. \\\n",
    "Always answer as helpfully as possible and follow ALL given instructions. \\\n",
    "Do not speculate or make up information. \\\n",
    "Do not reference any given instructions or context. \\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Functions with Updated Formatting\n",
    "def messages_to_prompt(messages: Sequence[ChatMessage], system_prompt: Optional[str] = None) -> str:\n",
    "    string_messages = []\n",
    "    if messages[0].role == MessageRole.SYSTEM:\n",
    "        system_message_str = messages[0].content or \"\"\n",
    "        messages = messages[1:]\n",
    "    else:\n",
    "        system_message_str = system_prompt or DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "    # Add system message at the start\n",
    "    system_message_str = f\"{IM_START}system\\n {system_message_str.strip()} {IM_END}\\n\"\n",
    "    string_messages.append(system_message_str)\n",
    "\n",
    "    for i in range(0, len(messages), 2):\n",
    "        user_message = messages[i]\n",
    "        assert user_message.role == MessageRole.USER\n",
    "        str_message = f\"{IM_START}user\\n {user_message.content}{IM_END}\\n\"\n",
    "\n",
    "        if len(messages) > (i + 1):\n",
    "            assistant_message = messages[i + 1]\n",
    "            assert assistant_message.role == MessageRole.ASSISTANT\n",
    "            str_message += f\"{IM_START}assistant\\n{assistant_message.content}{IM_END}\\n\"\n",
    "\n",
    "        string_messages.append(str_message)\n",
    "\n",
    "    return \"\".join(string_messages)\n",
    "\n",
    "\n",
    "def completion_to_prompt(completion: str, system_prompt: Optional[str] = None) -> str:\n",
    "    system_prompt_str = system_prompt or DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "    return (\n",
    "        f\"{IM_START}system\\n {system_prompt_str.strip()} {IM_END}\\n\"\n",
    "        f\"{IM_START}assistant\\n {completion.strip()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from llama_index import VectorStoreIndex, ServiceContext, SimpleDirectoryReader\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index import set_global_tokenizer\n",
    "from llama_index.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup global tokenizer\n",
    "set_global_tokenizer(AutoTokenizer.from_pretrained(\"jan-hq/stealth-v1.2\").encode)\n",
    "\n",
    "# System prompt template\n",
    "system_prompt = \"You are a helpful and careful assistant. You will use the given context to answer the multiple choice question. Only response 1 letter (A, B, C or D).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to setup LlamaCPP model\n",
    "def setup_llama_cpp(model_url, n_gpu_layers=100, context_window=512):\n",
    "    return LlamaCPP(\n",
    "        model_url=model_url,\n",
    "        model_path=None,\n",
    "        temperature=0.1,\n",
    "        max_new_tokens=3,\n",
    "        system_prompt=system_prompt,\n",
    "        context_window=context_window,\n",
    "        messages_to_prompt=messages_to_prompt,\n",
    "        completion_to_prompt=completion_to_prompt,        \n",
    "        model_kwargs={\"n_gpu_layers\": n_gpu_layers},\n",
    "        verbose=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to setup Service Context\n",
    "def setup_service_context(llm, embed_model_name, chunk_size=300, chunk_overlap=30):\n",
    "    embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "    return ServiceContext.from_defaults(llm=llm, embed_model=embed_model, chunk_size=chunk_size, chunk_overlap=chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"PATH/TO/YOUR/DOCUMENT/FOLDER\").load_data()\n",
    "\n",
    "# Setting up base and finetuned models\n",
    "model_url_base = \"https://huggingface.co/janhq/stealth-v1.2-GGUF/resolve/main/stealth-v1.2.Q4_K_M.gguf\"\n",
    "model_url_fintuned = \"https://huggingface.co/janhq/nitro-v1.2-e3-GGUF/resolve/main/nitro-v1.2-e3.Q4_K_M.gguf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_base = setup_llama_cpp(model_url_base)\n",
    "llm_finetuned = setup_llama_cpp(model_url_fintuned)\n",
    "\n",
    "service_context_base = setup_service_context(llm_base, \"BAAI/bge-base-en-v1.5\")\n",
    "service_context_fintuned = setup_service_context(llm_finetuned, \"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Create vector store index\n",
    "index_base = VectorStoreIndex.from_documents(documents, service_context=service_context_base)\n",
    "index_finetuned = VectorStoreIndex.from_documents(documents, service_context=service_context_fintuned)\n",
    "\n",
    "# Set up query engines\n",
    "query_engine_base = index_base.as_query_engine()\n",
    "query_engine_finetuned = index_finetuned.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to process the concatenated text with models\n",
    "def process_text(text):\n",
    "    response_base = query_engine_base.query(text)\n",
    "    response_finetuned = llm_finetuned.complete(text).text\n",
    "    response_rag_finetuned = query_engine_finetuned.query(text)\n",
    "    return response_base, response_finetuned, response_rag_finetuned\n",
    "\n",
    "# Read questions from CSV and process\n",
    "input_csv_file = 'mcq_nitro.csv'\n",
    "output_csv_file = 'model_responses.csv'\n",
    "\n",
    "responses = []\n",
    "\n",
    "# Revised loop for reading and processing CSV data\n",
    "try:\n",
    "    with open(input_csv_file, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in tqdm(reader, desc=\"Processing questions\"):\n",
    "            concatenated_text = '\\n'.join([row['question'], row['a'], row['b'], row['c'], row['d']])\n",
    "            base_response, finetuned_response, rag_finetuned_response = process_text(concatenated_text)\n",
    "            responses.append({\n",
    "                'Question': row['question'],\n",
    "                'Base Model': base_response,\n",
    "                'Finetuned Model': finetuned_response,\n",
    "                'RAG Finetuned Model': rag_finetuned_response\n",
    "            })\n",
    "except IOError:\n",
    "    print(\"Error reading input CSV file\")\n",
    "\n",
    "# Write responses to CSV\n",
    "try:\n",
    "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['Question', 'Base Model', 'Finetuned Model', 'RAG Finetuned Model'])\n",
    "        writer.writeheader()\n",
    "        for data in responses:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error while writing to CSV\")\n",
    "\n",
    "print(f\"Responses saved to {output_csv_file}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
