{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import openai\n",
    "from langchain.text_splitter import TokenTextSplitter, MarkdownHeaderTextSplitter\n",
    "from datasets import Dataset\n",
    "from time import sleep\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ROOT_DIR = \"\"\n",
    "OUTPUT_PATH = \"\"\n",
    "SYSTEM_FILE_PATH = \"\"\n",
    "YOUR_REPO_NAME = \"\"\n",
    "\n",
    "# Login\n",
    "openai.api_key = \"OPENAI_KEY\"\n",
    "login(token=\"HUGGINGFACE_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads and returns the content of a file.\n",
    "def read_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Generates a response from the ChatGPT model\n",
    "def generate_chatgpt_response(messages, temperature=0.5, model=\"gpt-4\", max_tokens=4096):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model, messages=messages, temperature=temperature, max_tokens=max_tokens\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Process Markdown files\n",
    "def process_markdown_file(file_path, markdown_splitter, text_splitter):\n",
    "    with open(file_path, 'r') as file:\n",
    "        markdown_document = file.read()\n",
    "    md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "    return [chunk for split in md_header_splits for chunk in text_splitter.split_documents([split])]\n",
    "\n",
    "# Extracts question and answer pairs from a given text\n",
    "def extract_qa_pairs(text):\n",
    "    qa_pairs = []\n",
    "    current_pair = {}\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        if line.startswith('\"question\": '):\n",
    "            current_pair['question'] = line.split('\"question\": ')[1].strip(' \",')\n",
    "        elif line.startswith('\"answer\": '):\n",
    "            current_pair['answer'] = line.split('\"answer\": ')[1].strip(' \",')\n",
    "            qa_pairs.append(current_pair)\n",
    "            current_pair = {}\n",
    "    return qa_pairs\n",
    "\n",
    "# Parse QnA pairs as JSON\n",
    "def parse_response(response):\n",
    "    try:\n",
    "        parsed_data = json.loads(response)\n",
    "        return parsed_data['qa_pairs']\n",
    "    except json.JSONDecodeError:\n",
    "        return extract_qa_pairs(response)\n",
    "\n",
    "# Create new column in the dataset\n",
    "def create_message(row):\n",
    "    return [{\"content\": row['question'], \"role\": \"user\"}, {\"content\": row['answer'], \"role\": \"assistant\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Script Logic\n",
    "HEADERS_TO_SPLIT = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\")]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=HEADERS_TO_SPLIT)\n",
    "text_splitter = TokenTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "\n",
    "all_chunks = []\n",
    "master_df = pd.DataFrame(columns=['question', 'answer', 'raw'])\n",
    "\n",
    "for subdir, dirs, files in os.walk(ROOT_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith('.md'):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            file_chunks = process_markdown_file(file_path, markdown_splitter, text_splitter)\n",
    "            all_chunks.extend(file_chunks)\n",
    "\n",
    "# Generating QnA pairs with GPT\n",
    "for _ in range(3):\n",
    "    for chunk in all_chunks:  # Limiting to the first 3 for brevity\n",
    "        conversation = [{'role': 'system', 'content': read_file(SYSTEM_FILE_PATH)},\n",
    "                        {'role': 'user', 'content': str(chunk)}]\n",
    "        response_verification = generate_chatgpt_response(conversation)\n",
    "        qa_pairs = parse_response(response_verification)\n",
    "        qa_df = pd.DataFrame(qa_pairs)\n",
    "        qa_df['raw'] = [chunk] * len(qa_df)\n",
    "        master_df = pd.concat([master_df, qa_df], ignore_index=True)\n",
    "        print(master_df.shape)\n",
    "\n",
    "master_df.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplication\n",
    "df_deduplicated = master_df.drop_duplicates()\n",
    "df_deduplicated['messages'] = df_deduplicated.apply(create_message, axis=1)\n",
    "\n",
    "# Convert data frame to Huggingface dataset format\n",
    "messages = df_deduplicated['message'].tolist()\n",
    "rejected = df_deduplicated['rejected'].tolist()\n",
    "hf_dataset = Dataset.from_dict({'messages': messages, 'chosen': messages, 'rejected': rejected})\n",
    "\n",
    "# Split train and test\n",
    "split_dataset = hf_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "split_dataset.push_to_hub(YOUR_REPO_NAME)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
